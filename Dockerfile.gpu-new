# Global build arguments
ARG NB_USER=notebook-user
ARG NB_UID=1000
ARG PIP_VERSION=23.2.1

# Use the official PyTorch image as the base. It comes with a known-good
# environment, including Python 3.11, CUDA 12.9, and the full development
# toolkit needed for torch.compile and building other dependencies.
FROM pytorch/pytorch:2.8.0-cuda12.9-cudnn9-runtime as base

# Avoid prompts from apt
ENV DEBIAN_FRONTEND=noninteractive

# Re-declare ARGs from the global scope to use them in this stage
ARG NB_USER
ARG NB_UID

# The pytorch image uses conda. The python executable is in /opt/conda/bin.
# We set our environment variables to point there directly.
ENV PYTHON /opt/conda/bin/python
ENV PIP /opt/conda/bin/python -m pip

# Install system dependencies. The base image is Ubuntu 22.04.
# g++ is needed for torch.compile's Triton backend.
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    g++ \
    libgl1-mesa-glx \
    libglib2.0-0 \
    poppler-utils \
    tesseract-ocr \
    tesseract-ocr-eng \
    libmagic1 \
    libreoffice \
    pandoc \
    && apt-get clean && rm -rf /var/lib/apt/lists/* \
    && ldconfig

# The base image runs as root, so we create our user.
RUN groupadd --gid ${NB_UID} ${NB_USER} && \
    useradd --uid ${NB_UID} --gid ${NB_UID} -m ${NB_USER}
ENV HOME /home/${NB_USER}
WORKDIR ${HOME}
USER ${NB_USER}

# The pytorch base image correctly sets the library paths, but we make it
# explicit to ensure onnxruntime can find the .so files.
ENV LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}

ENV PYTHONPATH="${PYTHONPATH}:${HOME}"
ENV PATH="/home/${NB_USER}/.local/bin:${PATH}"


# Stage 2: Install Python dependencies
FROM base as python-deps
ARG NB_USER
ARG PIP_VERSION

# Upgrade pip
RUN ${PIP} install --no-cache-dir pip==${PIP_VERSION}

# Copy the application requirements and local source code.
COPY --chown=${NB_USER}:${NB_USER} unstructured-api/requirements/gpu.txt requirements-gpu.txt
COPY --chown=${NB_USER}:${NB_USER} unstructured-inference/ ./unstructured-inference/
COPY --chown=${NB_USER}:${NB_USER} unstructured/ ./unstructured/

# The base image already has torch. Filter the requirements to avoid
# re-installing it, onnxruntime, or unstructured.
RUN grep -vE '^torch|^onnxruntime|^unstructured' requirements-gpu.txt > requirements-gpu-filtered.txt

# Install application dependencies, then finally fix the onnxruntime installation.
RUN ${PIP} install --no-cache-dir -e ./unstructured-inference/ && \
    ${PIP} install --no-cache-dir -e './unstructured[all-docs]' && \
    ${PIP} install --no-cache-dir -r requirements-gpu-filtered.txt && \
    # Rip out any CPU-only onnxruntime and then install the correct GPU version.
    ${PIP} uninstall -y onnxruntime onnxruntime-gpu && \
    ${PIP} install --no-cache-dir \
        --upgrade python-multipart \
        gunicorn==22.0.0 \
        onnxruntime-gpu==1.20.1 \
        opencv-python-headless==4.9.0.80 \
        python-doctr \
        --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/

# Add a verification step to ensure the TensorRT EP is available.
# This will fail the build if the installation was not successful.
RUN python3 -c "import onnxruntime as ort; assert 'TensorrtExecutionProvider' in ort.get_available_providers(), 'TensorRT Execution Provider not found in onnxruntime'"

# Stage 3: Download model weights
FROM python-deps as model-deps
ARG NB_USER

# Final check of the fully-installed environment
RUN python3 -c "import onnxruntime; assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers(), f'CUDA provider not found in {onnxruntime.get_available_providers()}'"

# Download the NLTK packages
ENV NLTK_DATA=${HOME}/nltk_data
RUN ${PYTHON} -m nltk.downloader -d ${NLTK_DATA} punkt_tab averaged_perceptron_tagger_eng

# Pre-download all the models used by the application.
ENV UNSTRUCTURED_HI_RES_MODEL_NAME=yolox
RUN ${PIP} install --no-cache-dir Pillow==10.4.0 && \
    ${PYTHON} -c "from unstructured.nlp.tokenize import download_nltk_packages; download_nltk_packages()" && \
    ${PYTHON} -c "from unstructured.partition.model_init import initialize; initialize()" && \
    # Force-clear the doctr cache to prevent corruption from parallel downloads
    rm -rf ${HOME}/.cache/doctr && \
    # Pre-download the specific doctr model used by the application to avoid runtime downloads.
    ${PYTHON} -c "from doctr.models import ocr_predictor; ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)"

# Stage 4: Final application stage
FROM model-deps as final
ARG NB_USER
USER ${NB_USER}
WORKDIR ${HOME}

# Ensure the container requests GPU access at runtime.
ENV NVIDIA_VISIBLE_DEVICES all
ENV NVIDIA_DRIVER_CAPABILITIES compute,utility

ENV OCR_AGENT=unstructured.partition.utils.ocr_models.doctr_ocr.OCRAgentDocTR
ENV UNSTRUCTURED_MEMORY_FREE_MINIMUM_MB=512
ENV WORKERS_PER_ENDPOINT=1

COPY --chown=${NB_USER}:${NB_USER} unstructured-api/sample-docs/ ./sample-docs/
COPY --chown=${NB_USER}:${NB_USER} unstructured-api/gpu_orchestrator/ ./gpu_orchestrator/
COPY --chown=${NB_USER}:${NB_USER} unstructured-api/scripts/ ./scripts/
COPY --chown=${NB_USER}:${NB_USER} unstructured-api/prepline_general/ ./prepline_general/
COPY --chown=${NB_USER}:${NB_USER} unstructured-api/logger_config.yaml .

RUN chmod +x scripts/*.sh

EXPOSE 8000
ENTRYPOINT ["scripts/manager-start.sh"]
